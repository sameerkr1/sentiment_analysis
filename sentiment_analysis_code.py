# -*- coding: utf-8 -*-
"""Mission SIH.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1qtWk_mvJO08JJa2E7appjlVnkImFzw2G
"""

from google.colab import drive
drive.mount('/content/drive/')

!pip install -U -q PyDrive

from pydrive.auth import GoogleAuth

from pydrive.drive import GoogleDrive

from google.colab import auth
from oauth2client.client import GoogleCredentials
#Authenticate and create the PyDrive client
auth.authenticate_user()
gauth = GoogleAuth()
gauth.credentials = GoogleCredentials.get_application_default()
drive = GoogleDrive(gauth)

# Commented out IPython magic to ensure Python compatibility.
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
# %matplotlib inline

link ='https://drive.google.com/open?id=1GVynEGExu3SGAM9C1PTz8DDoRs_958xg'
fluff, id = link.split('=')

downloaded = drive.CreateFile({'id':id})

downloaded.GetContentFile('split1.csv')

dataset = pd.read_csv("split1.csv")
dataset.head(5)

!pip install torch_nightly -f https://download.pytorch.org/whl/nightly/cu92/torch_nightly.html
!pip install fastai

import fastai
from fastai import *
from fastai.text import * 
import pandas as pd
import numpy as np
from functools import partial
import io
import os



dataset=dataset.rename(columns={"3":"Rating","more like funchuck":"Summary",'Gave this to my dad for a gag gift after directing "Nunsense," he got a reall kick out of it!':"Text"})
dataset.head()

dataset.shape

import matplotlib.pyplot as plt
dataset["Rating"].value_counts().plot(kind='pie',figsize=(6,6),autopct='%1.1f%%',shadow=True)
plt.show()

ds=dataset.drop_duplicates(inplace=False)
ds.shape
# No duplicates

dataset["Rating"].value_counts()

dataset=dataset.drop(columns="Summary",axis=1)

dataset.head(5)



KeepRating=pd.DataFrame(dataset["Rating"])
KeepRating.head()

def funForClass(row):
   if row['Rating'] == 1 or row['Rating']==2 :
      return 0
   if row['Rating'] == 3 :
      return 2
   if row['Rating'] == 4 or row['Rating']==5  :
      return 1

dataset['Class'] = dataset.apply (lambda row: funForClass(row), axis=1)

dataset['Class'].value_counts()
# 0->Neg
# 1->neutral
# 2->pos

dataset=dataset.drop(columns="Rating",axis=1)

print(dataset.shape)
dataset.head(15)

dataset=dataset.drop(dataset[dataset["Class"] == 2].index)
dataset.index = range(len(dataset))
print(dataset.shape)
dataset.head(15)

dataset['Class'].value_counts()

df=pd.concat([dataset['Class'],dataset['Text']],axis=1)
df.head(10)

df['Text'] = df['Text'].str.replace("[^a-zA-Z]", " ")
# df.head(10)

import nltk
nltk.download('stopwords')

from nltk.corpus import stopwords 
stop_words = stopwords.words('english')

# tokenization 
tokenized_doc = df['Text'].apply(lambda x: x.split())

# remove stop-words 
tokenized_doc = tokenized_doc.apply(lambda x: [item for item in x if item not in stop_words]) 

# de-tokenization 
detokenized_doc = [] 
for i in range(len(df)): 
    t = ' '.join(tokenized_doc[i]) 
    detokenized_doc.append(t) 
df['Text'] = detokenized_doc

df.head(10)

from sklearn.model_selection import train_test_split

# split data into training and validation set
df_trn, df_val = train_test_split(df, stratify = df['Class'], test_size = 0.2, random_state = 1002)

df_trn.shape, df_val.shape

# Language model data
data_lm = TextLMDataBunch.from_df(train_df = df_trn, valid_df = df_val, path = "")

learn = language_model_learner(data_lm,AWD_LSTM, drop_mult=0.1)

list(learn.model.children())

learn.lr_find(start_lr=1e-8, end_lr=1e2)
learn.recorder.plot()

learn = language_model_learner(data_lm, AWD_LSTM, 
                               drop_mult=0.5)
learn.fit_one_cycle(cyc_len=1, max_lr=1e-3, moms=(0.8, 0.7))

learn.unfreeze()
learn.fit_one_cycle(cyc_len=1, max_lr=1e-3, moms=(0.8, 0.7))

# Save the fine-tuned encoder
learn.save_encoder('ft_enc')





# data_lm.save()

# data_lm.show_batch()

# Classifier model data
data_clas = TextClasDataBunch.from_df(path = "", train_df = df_trn, valid_df = df_val, vocab=data_lm.train_ds.vocab, bs=32)
data_clas.save()

learn = text_classifier_learner(data_clas,AWD_LSTM, drop_mult=0.5)
learn.load_encoder('ft_enc')
learn.freeze()

learn.lr_find(start_lr=1e-8, end_lr=1e2)
learn.recorder.plot()

learn.fit_one_cycle(cyc_len=1, max_lr=1e-3, moms=(0.8, 0.7))

learn.freeze_to(-2)
learn.fit_one_cycle(1, slice(1e-4,1e-2), moms=(0.8,0.7))

learn.freeze_to(-3)
learn.fit_one_cycle(1, slice(1e-5,5e-3), moms=(0.8,0.7))

learn.unfreeze()
learn.metrics=[accuracy,
                Precision(average='macro'), 
                Recall(average='macro'), 
                FBeta(average='macro')
                               ]
learn.fit_one_cycle(1, slice(1e-5,1e-3), moms=(0.8,0.7))

# get predictions
preds, targets = learn.get_preds()
predictions = np.argmax(preds, axis=1)
pd.crosstab(predictions, targets)

x=learn.predict(item=" i know you make it worst,but it is good as well")
x

x



# learn = language_model_learner(data_lm,AWD_LSTM, drop_mult=0.7)

# learn.lr_find() # find learning rate
# learn.recorder.plot() # plot learning rate graph

# train the learner object
# learn.fit_one_cycle(1, 1e-2)

# learn.unfreeze()
# learn.lr_find()

# learn.recorder.plot()

# learn.save_encoder('ft_enc')

# learn = text_classifier_learner(data_clas,AWD_LSTM,drop_mult=0.7)
# learn.load_encoder('ft_enc')

# learn.lr_find()
# learn.recorder.plot()

# learn.metrics=[accuracy,
#                 Precision(average='macro'), 
#                 Recall(average='macro'), 
#                 FBeta(average='macro')
#                                ]

# learn.fit_one_cycle(1, 1e-2)

# # get predictions
# preds, targets = learn.get_preds()

# predictions = np.argmax(preds, axis = 1)
# pd.crosstab(predictions, targets)